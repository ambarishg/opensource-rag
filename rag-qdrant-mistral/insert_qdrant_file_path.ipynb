{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"../SustainabilityReports/\"\n",
    "\n",
    "# Model Settings\n",
    "MODEL_NAME=\"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Qdrant server URL\n",
    "URL =\"localhost\"\n",
    "# Qdrant dimension of the collection\n",
    "DIMENSION = 384\n",
    "# Qdrant collection name\n",
    "COLLECTION_NAME = \"SEARCH_ENGINE\"\n",
    "METRIC_NAME =\"COSINE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/Ambarish/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import qdrant_client as qc\n",
    "import qdrant_client.http.models as qmodels\n",
    "from qdrant_client.http.models import *\n",
    "\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the PDF file and return the text\n",
    "def get_pdf_data(file_path, num_pages = 1):\n",
    "    reader = PdfReader(file_path)\n",
    "    full_doc_text = \"\"\n",
    "    pages = reader.pages\n",
    "    num_pages = len(pages) \n",
    "    \n",
    "    try:\n",
    "        for page in range(num_pages):\n",
    "            current_page = reader.pages[page]\n",
    "            text = current_page.extract_text()\n",
    "            full_doc_text += text\n",
    "    except:\n",
    "        print(\"Error reading file\")\n",
    "    finally:\n",
    "        return full_doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the text into chunks of chunk_length \n",
    "# [ default is 500] characters\n",
    "def get_chunks(fulltext:str,chunk_length =500) -> list:\n",
    "    text = fulltext\n",
    "\n",
    "    chunks = []\n",
    "    while len(text) > chunk_length:\n",
    "        last_period_index = text[:chunk_length].rfind('.')\n",
    "        if last_period_index == -1:\n",
    "            last_period_index = chunk_length\n",
    "        chunks.append(text[:last_period_index])\n",
    "        text = text[last_period_index+1:]\n",
    "    chunks.append(text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qc.QdrantClient(url=URL)\n",
    "METRIC = qmodels.Distance.COSINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../SustainabilityReports/Nestle.pdf\n",
      "Full doc text length: 214517\n",
      "Full embeddings length: 517\n",
      "Inserting chunk 0 to 99\n",
      "Inserting chunk 100 to 199\n",
      "Inserting chunk 200 to 299\n",
      "Inserting chunk 300 to 399\n",
      "Inserting chunk 400 to 499\n",
      "Inserting chunk 500 to 516\n",
      "Processing file: ../SustainabilityReports/Ecology-From-Individuals-to-Ecosystems-by-Michael-Begon--2006-.pdf\n",
      "Full doc text length: 2926750\n",
      "Full embeddings length: 7010\n",
      "Inserting chunk 0 to 99\n",
      "Inserting chunk 100 to 199\n",
      "Inserting chunk 200 to 299\n",
      "Inserting chunk 300 to 399\n",
      "Inserting chunk 400 to 499\n",
      "Inserting chunk 500 to 599\n",
      "Inserting chunk 600 to 699\n",
      "Inserting chunk 700 to 799\n",
      "Inserting chunk 800 to 899\n",
      "Inserting chunk 900 to 999\n",
      "Inserting chunk 1000 to 1099\n",
      "Inserting chunk 1100 to 1199\n",
      "Inserting chunk 1200 to 1299\n",
      "Inserting chunk 1300 to 1399\n",
      "Inserting chunk 1400 to 1499\n",
      "Inserting chunk 1500 to 1599\n",
      "Inserting chunk 1600 to 1699\n",
      "Inserting chunk 1700 to 1799\n",
      "Inserting chunk 1800 to 1899\n",
      "Inserting chunk 1900 to 1999\n",
      "Inserting chunk 2000 to 2099\n",
      "Inserting chunk 2100 to 2199\n",
      "Inserting chunk 2200 to 2299\n",
      "Inserting chunk 2300 to 2399\n",
      "Inserting chunk 2400 to 2499\n",
      "Inserting chunk 2500 to 2599\n",
      "Inserting chunk 2600 to 2699\n",
      "Inserting chunk 2700 to 2799\n",
      "Inserting chunk 2800 to 2899\n",
      "Inserting chunk 2900 to 2999\n",
      "Inserting chunk 3000 to 3099\n",
      "Inserting chunk 3100 to 3199\n",
      "Inserting chunk 3200 to 3299\n",
      "Inserting chunk 3300 to 3399\n",
      "Inserting chunk 3400 to 3499\n",
      "Inserting chunk 3500 to 3599\n",
      "Inserting chunk 3600 to 3699\n",
      "Inserting chunk 3700 to 3799\n",
      "Inserting chunk 3800 to 3899\n",
      "Inserting chunk 3900 to 3999\n",
      "Inserting chunk 4000 to 4099\n",
      "Inserting chunk 4100 to 4199\n",
      "Inserting chunk 4200 to 4299\n",
      "Inserting chunk 4300 to 4399\n",
      "Inserting chunk 4400 to 4499\n",
      "Inserting chunk 4500 to 4599\n",
      "Inserting chunk 4600 to 4699\n",
      "Inserting chunk 4700 to 4799\n",
      "Inserting chunk 4800 to 4899\n",
      "Inserting chunk 4900 to 4999\n",
      "Inserting chunk 5000 to 5099\n",
      "Inserting chunk 5100 to 5199\n",
      "Inserting chunk 5200 to 5299\n",
      "Inserting chunk 5300 to 5399\n",
      "Inserting chunk 5400 to 5499\n",
      "Inserting chunk 5500 to 5599\n",
      "Inserting chunk 5600 to 5699\n",
      "Inserting chunk 5700 to 5799\n",
      "Inserting chunk 5800 to 5899\n",
      "Inserting chunk 5900 to 5999\n",
      "Inserting chunk 6000 to 6099\n",
      "Inserting chunk 6100 to 6199\n",
      "Inserting chunk 6200 to 6299\n",
      "Inserting chunk 6300 to 6399\n",
      "Inserting chunk 6400 to 6499\n",
      "Inserting chunk 6500 to 6599\n",
      "Inserting chunk 6600 to 6699\n",
      "Inserting chunk 6700 to 6799\n",
      "Inserting chunk 6800 to 6899\n",
      "Inserting chunk 6900 to 6999\n",
      "Inserting chunk 7000 to 7009\n",
      "Processing file: ../SustainabilityReports/Novartis.pdf\n",
      "Full doc text length: 375021\n",
      "Full embeddings length: 917\n",
      "Inserting chunk 0 to 99\n",
      "Inserting chunk 100 to 199\n",
      "Inserting chunk 200 to 299\n",
      "Inserting chunk 300 to 399\n",
      "Inserting chunk 400 to 499\n",
      "Inserting chunk 500 to 599\n",
      "Inserting chunk 600 to 699\n",
      "Inserting chunk 700 to 799\n",
      "Inserting chunk 800 to 899\n",
      "Inserting chunk 900 to 916\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for the chunks\n",
    "# Insert the chunks into the Qdrant collection\n",
    "# Insert the metadata for the chunks into the Qdrant collection\n",
    "FILES = os.listdir(FILE_PATH)\n",
    "FILES_FULL_PATH = [FILE_PATH + file for file in FILES]\n",
    "for filename in FILES_FULL_PATH:\n",
    "    print(f'Processing file: {filename}')\n",
    "    full_doc_text = get_pdf_data(filename)\n",
    "    print(f'Full doc text length: {len(full_doc_text)}')\n",
    "    payloads = []\n",
    "    li_id = []\n",
    "    corpus = []\n",
    "    Lines =get_chunks(full_doc_text,500)\n",
    "    for token in Lines:\n",
    "        corpus.append(token)\n",
    "        payloads.append({\"token\":token,\n",
    "                         \"filename\": os.path.basename(filename),\n",
    "                           \"type\":\"pdf\"})\n",
    "        li_id.append(str(uuid.uuid4()))\n",
    "    embeddings_all = model.encode(corpus, convert_to_tensor=True)\n",
    "    print(f'Full embeddings length: {len(embeddings_all)}')\n",
    "\n",
    "    CHUNK_SIZE = 100\n",
    "    for i in range(0, len(embeddings_all), CHUNK_SIZE):\n",
    "        if(i+CHUNK_SIZE > len(embeddings_all) -1):\n",
    "            new_chunk = len(embeddings_all) -1\n",
    "        else:\n",
    "            new_chunk = i+CHUNK_SIZE -1\n",
    "        print(\"Inserting chunk\", i , \"to\", new_chunk)\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=qmodels.Batch(\n",
    "                ids = li_id[i:new_chunk],\n",
    "                vectors=embeddings_all[i:new_chunk].tolist(),\n",
    "                payloads=payloads[i:new_chunk]\n",
    "            ),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
